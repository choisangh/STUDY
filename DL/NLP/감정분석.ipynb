{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "감정분석.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPuqse8rn4bVhv25PI8VHEu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choisangh/STUDY/blob/main/DL/NLP/%EA%B0%90%EC%A0%95%EB%B6%84%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ref : https://techblog-history-younghunjo1.tistory.com/111"
      ],
      "metadata": {
        "id": "VAFGDtJAdBhY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "O2ZFKemgNzsJ",
        "outputId": "1c23f923-c479-4297-baf0-536009055fb9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container{width:100% !important;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime as dt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "\n",
        "#-------------------- 차트 관련 속성 (한글처리, 그리드) -----------\n",
        "#plt.rc('font', family='NanumGothicOTF') # For MacOS\n",
        "plt.rcParams['font.family']= 'Malgun Gothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "sns.set()\n",
        "\n",
        "#-------------------- 주피터 , 출력결과 넓이 늘리기 ---------------\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container{width:100% !important;}</style>\"))\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('max_colwidth', None)\n",
        "\n",
        "#-------------------- 차트 관련 속성 (한글처리, 그리드) -----------\n",
        "plt.rcParams['font.family']= 'Malgun Gothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "qs5zAP5YOSgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# colab\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c5eogGhN1lV",
        "outputId": "2e540017-f27a-47cd-e83a-3e15c4750cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로드\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "# subset인자에 all로 train, test데이터 모두 가져오기\n",
        "news_data = fetch_20newsgroups(subset='all', random_state=42)\n",
        "# 데이터 분할\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "#remove인자에 있는 부분은 해당 기사의 일부분을 제거한 것이다. 왜냐하면 오로지 기사의 body(본문)에 해당하는 텍스트들로만 학습시키려 했기 때문\n",
        "train_data = fetch_20newsgroups(subset='train',\n",
        "                               remove=('headers','footers','quotes'),\n",
        "                               random_state=12)\n",
        "X_train = train_data.data\n",
        "y_train = train_data.target\n",
        "\n",
        "test_data = fetch_20newsgroups(subset='test',\n",
        "                              remove=('headers','footers','quotes'),\n",
        "                              random_state=12)\n",
        "X_test = test_data.data\n",
        "y_test = test_data.target\n",
        "print(f\"학습 데이터 크기: {len(X_train)}\\n 테스트 데이터 크기: {len(X_test)}\")"
      ],
      "metadata": {
        "id": "x-RtF7mbdPp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b5e5d3-6d34-4225-f73b-e48ddd864056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 데이터 크기: 11314\n",
            " 테스트 데이터 크기: 7532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 데이터 feature Vectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cnt_vect = CountVectorizer() #텍스트 벡터화\n",
        "# CountVecorizer도 한번 fit 하면 검증, 테스트 데이터에도 똑같은 것으로 해야함!\n",
        "# 다시 한 번 fit하게 되면 벡터화되는 feature 개수가 달라짐!\n",
        "X_train_vec = cnt_vect.fit_transform(X_train)\n",
        "X_test_vec = cnt_vect.transform(X_test)\n",
        "\n",
        "# 로지스틱 리그레션으로 모델링하기\n",
        "lr_clf = LogisticRegression() #분류\n",
        "lr_clf.fit(X_train_vec, y_train)\n",
        "y_pred = lr_clf.predict(X_test_vec)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"분류 정확도 : {acc : .4f}\")"
      ],
      "metadata": {
        "id": "1738cCSMdQcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b861490d-ac0b-42a3-9816-852e12daac52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "분류 정확도 :  0.6063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fd7CWe5%2FbtqGG1kV1lD%2F9M6ujgYrB6DLCeJuYvuIW0%2Fimg.png\"></img>"
      ],
      "metadata": {
        "id": "MGYqzfEKdc-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tf-idf로 feature vectorizer 시킨 후 로지스틱리그레션 해보기\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "tf_idf = TfidfVectorizer()\n",
        "X_train_tf = tf_idf.fit_transform(X_train)\n",
        "X_test_tf = tf_idf.transform(X_test)\n",
        "\n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_tf, y_train)\n",
        "y_pred = lr_clf.predict(X_test_tf)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Tf-idf 벡터화 후 정확도 :{acc : .4f}\")"
      ],
      "metadata": {
        "id": "FHFsQ-DOdh53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040fdb59-be5b-4531-fdda-7154fe495aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tf-idf 벡터화 후 정확도 : 0.6737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F9FHO4%2FbtqGK6ejxWD%2FcmoLoWtdk2BndsykK0ZQ00%2Fimg.png\"></img>"
      ],
      "metadata": {
        "id": "QBFErGcxdk0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 리뷰 감성 분류하기"
      ],
      "metadata": {
        "id": "9qE60vobdsin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-1. 지도학습에 기반한 감성 분석"
      ],
      "metadata": {
        "id": "fN3hZ4CEeCXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline을 통해서 텍스트를 벡터화시키고 모델 학습시키기!\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# tsv 파일도 csv호출함수로 불러올 수 있음!\n",
        "# headers=0이면 헤더값(칼럼이름) 출력, 1이면 헤더값 없이 바로 출력\n",
        "# quoting=3 이면 \"\"(큰따음포) 인용구는 무시하고 출력함!\n",
        "review_df = pd.read_csv('labeledTrainData.tsv', encoding='utf-8',\n",
        "                       header=0, sep='\\t', quoting=3)\n",
        "# 정규표현식으로 HTML태그와 숫자 삭제하기\n",
        "# Series의 replace함수로 HTML태그 없애주기\n",
        "review_df['review'] = review_df['review'].str.replace('<br />',' ')\n",
        "# re.sub('패턴','뭘로바꿀지',바꿀문자열)\n",
        "review_df['review'] = review_df['review'].apply(lambda x : re.sub('[^a-zA-Z]',' ',x))\n",
        "\n",
        "# 학습, 테스트 데이터 분리\n",
        "target = review_df['sentiment']\n",
        "feature = review_df['review']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature,\n",
        "                                                   target,\n",
        "                                                   test_size=0.3,\n",
        "                                                   random_state=42)\n",
        "# Vectorizer인자로는 불용어 제거, ngram범위 설정해주기\n",
        "pipeline = Pipeline([('cnt_vect',CountVectorizer(stop_words='english',\n",
        "                                               ngram_range=(1,2))),\n",
        "                    ('lr_clf', LogisticRegression(C=10))])\n",
        "# 위에서 설정해준 Pipleline으로 데이터 학습시킬 때는 벡터화시키기 전의 원본 데이터\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "# '1' label로 예측될 확률 추출 for roc_auc_score\n",
        "y_pred_proba = pipeline.predict_proba(X_test)[:,1]\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"정확도:{acc : .3f}\\nAUC score:{auc : .3f}\")         "
      ],
      "metadata": {
        "id": "0arPZp1mdpu-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "51c25290-3f29-4090-cf05-a3691d437ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6a2b31eae07b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# quoting=3 이면 \"\"(큰따음포) 인용구는 무시하고 출력함!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m review_df = pd.read_csv('labeledTrainData.tsv', encoding='utf-8',\n\u001b[0;32m---> 14\u001b[0;31m                        header=0, sep='\\t', quoting=3)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# 정규표현식으로 HTML태그와 숫자 삭제하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Series의 replace함수로 HTML태그 없애주기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'labeledTrainData.tsv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbWCisd%2FbtqGG0GiRXb%2Fik6hfgIrnBjVe3H6ZHUK00%2Fimg.png\">"
      ],
      "metadata": {
        "id": "vdBcshOud3VH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-2. 비지도 학습기반 감성분석\n",
        "* 감성 사전 필요"
      ],
      "metadata": {
        "id": "0FiytLSad-w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer  #nltk에서 제공하는 감성 사전 <영어만 가능>\n",
        "\n",
        "senti_analyzer = SentimentIntensityAnalyzer()\n",
        "# polarity_score로 하나의 텍스트에 대해 각 부정/객관/긍정 그리고 총 합한 감성지수 출력(-1~1)\n",
        "senti_score = senti_analyzer.polarity_scores(review_df['review'][0])\n",
        "print(senti_score)"
      ],
      "metadata": {
        "id": "D3VNeRVDdv_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
        "from sklearn.metrics import recall_score, f1_score\n",
        "\n",
        "# 임계치설정(보통 0.1)을 통해 compound(총 감성지수)가 임계치값보다 높으면 긍정, 낮으면 부정으로 분석\n",
        "def get_sentiment(review, threshold):\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    scores = analyzer.polarity_scores(review)\n",
        "    \n",
        "    compound_score = scores['compound']\n",
        "    final_sentiment = 1 if compound_score >= threshold else 0\n",
        "    return final_sentiment\n",
        "\n",
        " # 각 텍스트 데이터에 위에서 설정한 감성 label 얻는 함수 적용하기\n",
        "# 임계값은 0.1로 설정\n",
        "review_df['vader_pred'] = review_df['review'].apply(lambda x : get_sentiment(x, 0.1))\n",
        "# 원본 데이터에서 주어진 정답 label과 VADER로 예측한 label 비교\n",
        "y_target = review_df['sentiment']\n",
        "y_pred = review_df['vader_pred']\n",
        "\n",
        "print(confusion_matrix(y_target, y_pred))\n",
        "print(\"정확도 :\", accuracy_score(y_target, y_pred))\n",
        "print(\"정밀도 :\", precision_score(y_target, y_pred))\n",
        "print(\"재현율 :\", recall_score(y_target, y_pred))\n",
        "print(\"F1 score :\", f1_score(y_target, y_pred))"
      ],
      "metadata": {
        "id": "XhXjvRhed84F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}